# -*- coding: utf-8 -*-
"""SAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A000BTTyJBpbdu3HH5HAXx4UXhLNpCdN
"""

import zipfile

with zipfile.ZipFile('Saliency4asd.zip', 'r') as zip_ref:
    zip_ref.extractall('Saliency4asd')

!git clone  https://github.com/marcellacornia/sam.git

# Commented out IPython magic to ensure Python compatibility.
# %cd sam
!pip install scikit-learn scipy tensorboard tqdm torchSummaryX

!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import numpy as np
import os
import zipfile
import matplotlib.pyplot as plt

# Specify device
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"üî• Using device: {DEVICE}")

# Define paths
sam_model_path = "/content/sam/sam_vit_h_4b8939.pth"  # Path to pre-trained model
image_folder = "/content/Saliency4asd/Saliency4asd/Images"  # Input image directory
output_folder = "/content/saliency_maps_sam"  # Output folder

# Ensure output directory exists
os.makedirs(output_folder, exist_ok=True)

# Define SAM model structure
class SAM(nn.Module):
    def __init__(self):
        super(SAM, self).__init__()
        # First convolutional block
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # Attention mechanism
        self.attention_conv1 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.attention_relu = nn.ReLU(inplace=True)
        self.attention_conv2 = nn.Conv2d(64, 1, kernel_size=1)

        # Final activation
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Feature extraction
        x = self.maxpool(self.relu1(self.conv1(x)))

        # Attention mechanism
        x = self.attention_relu(self.attention_conv1(x))
        x = self.attention_conv2(x)

        # Final activation
        x = self.sigmoid(x)
        return x

# Robust model loading function
def load_sam():
    model = SAM().to(DEVICE)
    try:
        # Try multiple loading methods
        try:
            # First try standard loading
            state_dict = torch.load(sam_model_path, map_location=DEVICE)
        except:
            # If that fails, try with weights_only=False
            state_dict = torch.load(sam_model_path, map_location=DEVICE, weights_only=False)

        # Handle potential state_dict nesting
        if 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']
        if 'model' in state_dict:
            state_dict = state_dict['model']

        # Clean state_dict keys if needed (remove 'module.' prefix if present)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}

        model.load_state_dict(state_dict, strict=False)
        print("‚úÖ SAM model loaded successfully")
    except Exception as e:
        print(f"‚ùå Error loading SAM model: {e}")
        print("Possible solutions:")
        print("1. Verify the model file is not corrupted")
        print("2. Check if the model architecture matches the saved weights")
        print("3. Try redownloading the model file")
        exit(1)
    model.eval()
    return model

# [Rest of your original code remains exactly the same...]
def preprocess_image(image):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0).to(DEVICE)

def generate_saliency_map(model, image):
    image_tensor = preprocess_image(image)
    with torch.no_grad():
        saliency_map = model(image_tensor)
    return saliency_map

def save_saliency_map(saliency_map, image_name):
    saliency_map_image = saliency_map.squeeze().cpu().numpy()
    saliency_map_image = (saliency_map_image - saliency_map_image.min()) / (saliency_map_image.max() - saliency_map_image.min())

    plt.imshow(saliency_map_image, cmap='hot')
    plt.colorbar()
    plt.title(f'SAM Saliency Map for {image_name}')

    save_path = os.path.join(output_folder, f"sam_{image_name}")
    plt.savefig(save_path)
    plt.close()
    return save_path

def download_saliency_maps():
    zip_path = "/content/saliency_maps_sam.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for file in os.listdir(output_folder):
            if file.endswith(".png"):
                file_path = os.path.join(output_folder, file)
                zipf.write(file_path, os.path.relpath(file_path, output_folder))
    try:
        from google.colab import files
        files.download(zip_path)
        print(f"‚úÖ SAM saliency maps downloaded as {zip_path}")
    except ImportError:
        print(f"Zip file created at {zip_path}. Manually download from file manager.")

def main(image_directory):
    model = load_sam()
    for filename in os.listdir(image_directory):
        if filename.lower().endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(image_directory, filename)
            image = Image.open(image_path).convert("RGB")
            saliency_map = generate_saliency_map(model, image)
            save_path = save_saliency_map(saliency_map, filename)
            print(f"‚úÖ Saved SAM saliency map: {save_path}")
    download_saliency_maps()

if __name__ == '__main__':
    main(image_folder)

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import torch
import torch.nn as nn
from torchvision import transforms

# Define folder paths
saliency_folder = "/content/saliency_maps_sam"
td_folder = "/content/Saliency4asd/Saliency4asd/TD_FixMaps"
asd_folder = "/content/Saliency4asd/Saliency4asd/ASD_FixMaps"
image_folder = "/content/Saliency4asd/Saliency4asd/Images"
sam_model_path = "/content/sam/sam_vit_h_4b8939.pth"

# Ensure output directory exists
os.makedirs(saliency_folder, exist_ok=True)

# Define device
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"üî• Using device: {DEVICE}")

# Define SAM model structure
class SAM(nn.Module):
    def __init__(self):
        super(SAM, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.attention_conv1 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.attention_relu = nn.ReLU(inplace=True)
        self.attention_conv2 = nn.Conv2d(64, 1, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.maxpool(self.relu1(self.conv1(x)))
        x = self.attention_relu(self.attention_conv1(x))
        x = self.attention_conv2(x)
        return self.sigmoid(x)

# ‚úÖ Function to load and initialize SAM model
def load_sam_model():
    model = SAM().to(DEVICE)
    try:
        state_dict = torch.load(sam_model_path, map_location=DEVICE)
        if 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']
        model.load_state_dict(state_dict, strict=False)
        print("‚úÖ SAM model loaded successfully")
    except Exception as e:
        print(f"‚ùå Error loading SAM model: {e}")
        exit(1)
    model.eval()
    return model

# ‚úÖ Function to preprocess image
def preprocess_image(image):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0).to(DEVICE)

# ‚úÖ Function to generate saliency map
def generate_saliency_map(model, image_path):
    image = Image.open(image_path).convert("RGB")
    image_tensor = preprocess_image(image)
    with torch.no_grad():
        saliency_map = model(image_tensor)
    return saliency_map.squeeze().cpu().numpy()

# ‚úÖ Function to save saliency map
def save_saliency_map(saliency_map, image_name):
    saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())
    plt.imshow(saliency_map, cmap='hot')
    plt.axis('off')
    save_path = os.path.join(saliency_folder, f"sam_{os.path.basename(image_name)}")
    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)
    plt.close()
    return save_path

# ‚úÖ Function to load fixation maps as numpy arrays
def load_fixation_map(map_path):
    """Loads fixation map as a numpy array."""
    if not map_path or not os.path.exists(map_path):
        return None
    fixation_map = Image.open(map_path).convert("L")
    return np.array(fixation_map)

# ‚úÖ Function to resize maps to match the saliency map size
def resize_map(fixation_map, target_shape):
    """Resizes fixation map to match target shape (height, width)."""
    if fixation_map is None:
        return None
    return cv2.resize(fixation_map, (target_shape[1], target_shape[0]))

# ‚úÖ Function to find the best matching fixation map
def find_best_match(image_name, folder):
    """Finds the best match for the given image name (ignoring extensions)."""
    base_name = os.path.splitext(image_name)[0]
    for file in os.listdir(folder):
        if file.startswith(base_name) and file.lower().endswith(('.png', '.jpg', '.jpeg')):
            return os.path.join(folder, file)
    return None

# ‚úÖ Function to visualize and compare saliency maps with fixation maps
def visualize_comparison(td_map, asd_map, saliency_map, image_name):
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    if td_map is not None:
        axs[0].imshow(td_map, cmap='hot')
        axs[0].set_title(f"TD Fixation Map: {image_name}")
    else:
        axs[0].axis('off')
        axs[0].set_title("No TD Fixation Map")

    if asd_map is not None:
        axs[1].imshow(asd_map, cmap='hot')
        axs[1].set_title(f"ASD Fixation Map: {image_name}")
    else:
        axs[1].axis('off')
        axs[1].set_title("No ASD Fixation Map")

    axs[2].imshow(saliency_map, cmap='hot')
    axs[2].set_title(f"Generated Saliency Map: {image_name}")

    for ax in axs:
        ax.axis("off")

    plt.show()

# Main processing function
def main():
    # Load SAM model
    model = load_sam_model()

    # First generate all saliency maps
    print("‚è≥ Generating saliency maps...")
    for image_name in os.listdir(image_folder):
        if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):
            image_path = os.path.join(image_folder, image_name)
            saliency_map = generate_saliency_map(model, image_path)
            save_saliency_map(saliency_map, image_name)
            print(f"‚úÖ Generated saliency map for {image_name}")

    # Then compare with fixation maps
    print("\n‚è≥ Comparing with fixation maps...")
    for saliency_name in os.listdir(saliency_folder):
        saliency_path = os.path.join(saliency_folder, saliency_name)

        if not os.path.isfile(saliency_path) or not saliency_name.lower().endswith(('.png', '.jpg', '.jpeg')):
            continue

        # Load saliency map
        saliency_map = np.array(Image.open(saliency_path).convert("L"))

        # Find matching fixation maps
        base_name = saliency_name.replace('sam_', '')
        td_path = find_best_match(base_name, td_folder)
        asd_path = find_best_match(base_name, asd_folder)

        # Load and resize fixation maps
        td_map = resize_map(load_fixation_map(td_path), saliency_map.shape)
        asd_map = resize_map(load_fixation_map(asd_path), saliency_map.shape)

        if td_map is None and asd_map is None:
            print(f"‚ö†Ô∏è Skipping {saliency_name}: No matching fixation maps")
            continue

        # Display comparison
        visualize_comparison(td_map, asd_map, saliency_map, base_name)

    print("‚úÖ All processing complete!")

if __name__ == '__main__':
    main()

!git clone https://github.com/cvzoya/saliency.git

!git clone https://github.com/matthias-k/saliency-benchmarking.git

import numpy as np
from sklearn.metrics import roc_auc_score
import scipy.stats
import cv2
from scipy.stats import entropy, pearsonr
from PIL import Image
import pandas as pd

# AUC Calculation (Borji, Judd, Shuffled)
def calculate_auc(y_true, y_pred):
    return roc_auc_score(y_true.flatten(), y_pred.flatten())

def calculate_auc_judd(y_true, y_pred):
    thresholds = np.linspace(0, 1, 20)
    scores = [roc_auc_score(y_true.flatten(), (y_pred >= t).astype(np.uint8).flatten()) for t in thresholds]
    return np.mean(scores)

def calculate_auc_shuffled(y_true, y_pred, random_fixations):
    return roc_auc_score(y_true.flatten(), y_pred.flatten()) - roc_auc_score(random_fixations.flatten(), y_pred.flatten())

# Pearson Correlation (CC)
def calculate_cc(y_true, y_pred):
    return pearsonr(y_true.flatten(), y_pred.flatten())[0]

# Mean Squared Error (MSE)
def calculate_mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Earth Mover's Distance (EMD)
def calculate_emd(y_true, y_pred):
    y_true = y_true.astype(np.uint8)
    y_pred = y_pred.astype(np.uint8)
    hist_true = cv2.calcHist([y_true], [0], None, [256], [0, 256])
    hist_pred = cv2.calcHist([y_pred], [0], None, [256], [0, 256])
    return cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_BHATTACHARYYA)

# Kullback-Leibler Divergence (KLdiv)
def calculate_kl_div(y_true, y_pred):
    p = np.histogram(y_true.flatten(), bins=256, range=(0, 256))[0] + 1e-10
    q = np.histogram(y_pred.flatten(), bins=256, range=(0, 256))[0] + 1e-10
    p = p / p.sum()
    q = q / q.sum()
    return entropy(p, q)

# Normalized Scanpath Saliency (NSS)
def calculate_nss(y_true, y_pred):
    y_pred = (y_pred - np.mean(y_pred)) / (np.std(y_pred) + 1e-10)
    return np.mean(y_true * y_pred)

# Information Gain (Info Gain)
def calculate_info_gain(y_true, y_pred, baseline):
    return calculate_kl_div(y_true, y_pred) - calculate_kl_div(y_true, baseline)

# Load and resize image
def load_and_resize_image(path, target_shape=(224, 224)):
    img = Image.open(path).convert('L')
    img_resized = img.resize(target_shape, Image.LANCZOS)
    return np.array(img_resized).astype(np.float32)

# Load saliency and fixation maps
saliency_map = load_and_resize_image('/content/saliency_maps_sam/sam_100.png')
td_fix_map = load_and_resize_image('/content/Saliency4asd/Saliency4asd/TD_FixMaps/100_s.png')
asd_fix_map = load_and_resize_image('/content/Saliency4asd/Saliency4asd/ASD_FixMaps/100_s.png')
random_fix_map = np.random.randint(0, 2, td_fix_map.shape)  # Generate random fixations

# Convert fixation maps to binary
td_fix_map = (td_fix_map > 0).astype(np.uint8)
asd_fix_map = (asd_fix_map > 0).astype(np.uint8)

# Normalize saliency map
td_saliency_map = (saliency_map - np.min(saliency_map)) / (np.max(saliency_map) - np.min(saliency_map))

# Compute metrics for TD vs Saliency
td_auc = calculate_auc(td_fix_map, td_saliency_map)
td_auc_judd = calculate_auc_judd(td_fix_map, td_saliency_map)
td_auc_shuffled = calculate_auc_shuffled(td_fix_map, td_saliency_map, random_fix_map)
td_cc = calculate_cc(td_fix_map, td_saliency_map)
td_mse = calculate_mse(td_fix_map, td_saliency_map)
td_emd = calculate_emd(td_fix_map, td_saliency_map)
td_kldiv = calculate_kl_div(td_fix_map, td_saliency_map)
td_nss = calculate_nss(td_fix_map, td_saliency_map)
td_info_gain = calculate_info_gain(td_fix_map, td_saliency_map, random_fix_map)

# Compute metrics for ASD vs Saliency
asd_auc = calculate_auc(asd_fix_map, td_saliency_map)
asd_auc_judd = calculate_auc_judd(asd_fix_map, td_saliency_map)
asd_auc_shuffled = calculate_auc_shuffled(asd_fix_map, td_saliency_map, random_fix_map)
asd_cc = calculate_cc(asd_fix_map, td_saliency_map)
asd_mse = calculate_mse(asd_fix_map, td_saliency_map)
asd_emd = calculate_emd(asd_fix_map, td_saliency_map)
asd_kldiv = calculate_kl_div(asd_fix_map, td_saliency_map)
asd_nss = calculate_nss(asd_fix_map, td_saliency_map)
asd_info_gain = calculate_info_gain(asd_fix_map, td_saliency_map, random_fix_map)

# Prepare results for CSV
results = {
    "Metric": ["AUC_Borji", "AUC_Judd", "AUC_Shuffled", "CC", "MSE", "EMD", "KLdiv", "NSS", "Info Gain"],
    "TD vs Saliency": [td_auc, td_auc_judd, td_auc_shuffled, td_cc, td_mse, td_emd, td_kldiv, td_nss, td_info_gain],
    "ASD vs Saliency": [asd_auc, asd_auc_judd, asd_auc_shuffled, asd_cc, asd_mse, asd_emd, asd_kldiv, asd_nss, asd_info_gain]
}

df = pd.DataFrame(results)
df.to_csv("/content/saliency_metrics.csv", index=False)

# Print Results
print("TD vs Saliency Metrics:")
print(f"AUC_Borji: {td_auc}, AUC_Judd: {td_auc_judd}, AUC_Shuffled: {td_auc_shuffled}, CC: {td_cc}, MSE: {td_mse}, EMD: {td_emd}, KLdiv: {td_kldiv}, NSS: {td_nss}, Info Gain: {td_info_gain}")

print("\nASD vs Saliency Metrics:")
print(f"AUC_Borji: {asd_auc}, AUC_Judd: {asd_auc_judd}, AUC_Shuffled: {asd_auc_shuffled}, CC: {asd_cc}, MSE: {asd_mse}, EMD: {asd_emd}, KLdiv: {asd_kldiv}, NSS: {asd_nss}, Info Gain: {asd_info_gain}")

print("\nMetrics saved to CSV: /content/saliency_metrics.csv")